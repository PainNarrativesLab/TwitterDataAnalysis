{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is concerned with analyzing the co-occurrance networks of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T03:03:52.599495Z",
     "start_time": "2018-06-20T03:03:51.501192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:twitteranalysis) -> /Users/adam/Dropbox/PainNarrativesLab/TwitterDataAnalysis\n",
      "/Users/adam/Dropbox/PainNarrativesLab/TwitterDataAnalysis\n"
     ]
    }
   ],
   "source": [
    "%cd twitteranalysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "#Plotting \n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import environment\n",
    "from DataTools import DataRepositories as DR\n",
    "from DataTools import DataConnections as DC\n",
    "from DataTools.WordORM import Word\n",
    "from DataTools.TweetORM import Users as User\n",
    "from DataTools.TweetORM import Tweet\n",
    "from SearchTools.WordMaps import get_adjacent_word_counts, get_adjacent_words, get_user_ids_for_word\n",
    "\n",
    "def make_term_ids_filepath(term, path=environment.LOG_FOLDER_PATH):\n",
    "    return \"%s/temp_output/tweet-ids/%s-ids.csv\" % (path, term)\n",
    "\n",
    "EXP_TERMS_FILEPATH = '%s/experimental-terms.xlsx' % environment.EXPERIMENTS_FOLDER\n",
    "IDS_FILEPATH = \"%s/temp_output/user-ids.xlsx\" % environment.LOG_FOLDER_PATH\n",
    "\n",
    "\n",
    "# load in terms to search for\n",
    "experimentalTerms = pd.read_excel(EXP_TERMS_FILEPATH, sheet_name='terms', squeeze=True)\n",
    "termMap = pd.read_excel(EXP_TERMS_FILEPATH, sheet_name='mapping')\n",
    "\n",
    "terms = [t for t in termMap.T.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-20T03:03:52.662145Z",
     "start_time": "2018-06-20T03:03:52.602671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating connection: mysql twitter_data\n"
     ]
    }
   ],
   "source": [
    "e = DC.initialize_engine('mysql')\n",
    "dao = DC.DAO(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T00:08:13.296218Z",
     "start_time": "2018-06-19T00:08:13.292012Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_term_network(terms):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples containing unique, unordered combinations\n",
    "    of two terms\n",
    "    terms = ['a', 'b', 'c']\n",
    "    expect = [ ('a', 'b'), ('a', 'c'),('b', 'c')]\n",
    "    assert(create_term_network(terms) == expect)\n",
    "    \n",
    "    \"\"\"\n",
    "    network = []\n",
    "    i = 0\n",
    "    for term in terms:\n",
    "        i +=1\n",
    "        [network.append((term, term2)) for term2 in terms[i:]]\n",
    "        \n",
    "    return network\n",
    "        \n",
    "\n",
    "terms = ['a', 'b', 'c']\n",
    "expect = [ ('a', 'b'), ('a', 'c'),('b', 'c')]\n",
    "assert(create_term_network(terms) == expect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtag co-occurrance\n",
    "\n",
    "Finding co-occurrance networks in the separately stored hashtags and tweetsXtags tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T19:25:28.907691Z",
     "start_time": "2018-06-12T19:25:28.905490Z"
    }
   },
   "source": [
    "## Determine whether a term has been used as a hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T22:52:52.956426Z",
     "start_time": "2018-06-18T22:52:52.923690Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_hashtag_id(term):\n",
    "    query = \"\"\"SELECT tagID FROM hashtags WHERE hashtag = %s\"\"\" % term\n",
    "\n",
    "def get_count_of_tweets_containing_hashtag(term):\n",
    "    query = \"\"\"SELECT h.hashtag, h.tagID, count(DISTINCT x.tweetID) AS tweetCount \n",
    "    FROM hashtags h \n",
    "    INNER JOIN tweetsXtags x ON (h.tagID = x.tagID)\n",
    "    WHERE h.hashtag = %s;\"\"\" % term\n",
    "\n",
    "def get_count_of_tweets_containing_two_hashtags(term1, term2):\n",
    "    query = \"\"\"\n",
    "        SELECT count(DISTINCT x.tweetID) \n",
    "        FROM tweetsXtags x \n",
    "        INNER JOIN (\n",
    "            SELECT x1.tweetID AS id \n",
    "                FROM tweetsXtags x1 \n",
    "                INNER JOIN (\n",
    "                    SELECT tagID AS id \n",
    "                    FROM hashtags \n",
    "                    WHERE hashtag = %s\n",
    "                    ) AS t1 \n",
    "                    ON (x1.tagID = t1.id)\n",
    "            ) AS t3\n",
    "            ON x.tweetID = t3.id\n",
    "        INNER JOIN (\n",
    "          SELECT x2.tweetID AS id \n",
    "                FROM tweetsXtags x2 \n",
    "                INNER JOIN (\n",
    "                    SELECT tagID AS id \n",
    "                    FROM hashtags \n",
    "                    WHERE hashtag = %s\n",
    "                ) AS t1 \n",
    "                ON (x2.tagID = t1.id)\n",
    "        ) AS t4\n",
    "        ON x.tweetID = t4.id \"\"\" % (term1, term2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T23:14:38.351529Z",
     "start_time": "2018-06-18T23:14:38.334994Z"
    }
   },
   "outputs": [],
   "source": [
    "create_node_network(experimentalTerms.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet word_map co-occcurrance\n",
    "\n",
    "Finding co-occurance networks in the tweet data stored in word_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T00:11:57.465101Z",
     "start_time": "2018-06-19T00:11:57.459756Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_tweet_id_data(term):\n",
    "    return pd.read_csv(make_term_ids_filepath(term), names=['idx', 'id']) #.drop('idx', axis=1)\n",
    "\n",
    "def get_cooccurences(term1, term2):\n",
    "    d1 = load_tweet_id_data(term1)\n",
    "    d2 = load_tweet_id_data(term2)\n",
    "    return (term1, term2, len(d1[d1.id.isin(d2.id)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T00:23:51.040082Z",
     "start_time": "2018-06-19T00:23:51.036757Z"
    }
   },
   "outputs": [],
   "source": [
    "terms = [t for t in termMap.T.index]\n",
    "network = create_term_network(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T00:24:05.410655Z",
     "start_time": "2018-06-19T00:23:51.642239Z"
    }
   },
   "outputs": [],
   "source": [
    "edges = [get_cooccurences(t1, t2) for t1, t2 in network]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T00:24:20.628179Z",
     "start_time": "2018-06-19T00:24:20.621493Z"
    }
   },
   "outputs": [],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make networkx graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T02:06:58.247489Z",
     "start_time": "2018-06-19T02:06:58.243593Z"
    }
   },
   "outputs": [],
   "source": [
    "GRAPHS_FOLDER = \"%s/temp_output/graphs\" % environment.LOG_FOLDER_PATH\n",
    "GRAPH_FILEPATH = \"%s/experimental-terms-in-tweets.gexf\" % GRAPHS_FOLDER\n",
    "\n",
    "def write_graph_to_file(graph, filepath=GRAPH_FILEPATH):\n",
    "    # write to file\n",
    "    nx.write_gexf(graph, filepath)\n",
    "\n",
    "@nx.utils.decorators.open_file(0,'r')\n",
    "def load_graph_from_file(filepath=GRAPH_FILEPATH):\n",
    "    return nx.read_gexf(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make graph file for gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T16:27:25.293435Z",
     "start_time": "2018-06-19T16:27:25.285978Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true,
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# This produces a network which is small enough that gephi doesn't die\n",
    "# It does not get the degree of nodes right since the number of edges is\n",
    "# stored as the weight of a single edge. But it seems to be okay for\n",
    "# visualizatinos\n",
    "G = nx.Graph()   # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
    "G.add_weighted_edges_from(edges)\n",
    "write_graph_to_file(G, '%s/exp-terms-in-tweets-for-gephi.gexf' % GRAPHS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make full graph file \n",
    "\n",
    "The output of this will be very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T01:44:09.456970Z",
     "start_time": "2018-06-19T01:44:07.168011Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# This produces a graph\n",
    "G = nx.MultiGraph()   # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
    "for n1, n2, degree in edges: \n",
    "    G.add_edges_from([(n1, n2) for i in range(0, degree)])\n",
    "G.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T01:46:24.768796Z",
     "start_time": "2018-06-19T01:46:08.235124Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "write_graph_to_file(G, 'experimental-terms-in-tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T01:57:51.877536Z",
     "start_time": "2018-06-19T01:57:31.174097Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "nx.draw_networkx(G, labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from gexf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T16:30:08.612136Z",
     "start_time": "2018-06-19T16:29:58.505297Z"
    }
   },
   "outputs": [],
   "source": [
    "g = load_graph_from_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T16:11:20.833626Z",
     "start_time": "2018-06-19T16:11:20.828699Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_nx_output_to_dataframe(output, measure_label):\n",
    "    \"\"\"\n",
    "    Networkx algorithms output a dictionary of results. \n",
    "    This converts them into a dataframe\n",
    "    Example:\n",
    "        dc = nx.degree_centrality(G)\n",
    "        dc = convert_nx_output_to_dataframe(dc, 'degree_centrality')\n",
    "    \"\"\"\n",
    "    j = []\n",
    "    for r in output.keys():\n",
    "        j.append({'term' : r, measure_label : output[r]})\n",
    "    j = pd.DataFrame(j)\n",
    "    j.set_index('term', inplace=True)\n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T16:20:06.228068Z",
     "start_time": "2018-06-19T16:20:06.218508Z"
    }
   },
   "outputs": [],
   "source": [
    "d = nx.degree(g)\n",
    "degree = pd.DataFrame([{ 'term' : term, 'degree': degree} for term, degree in d]).set_index('term')\n",
    "degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T16:23:10.443887Z",
     "start_time": "2018-06-19T16:23:10.227409Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(7, 4))\n",
    "degree.plot(kind='barh', ax=axes)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T16:22:30.073745Z",
     "start_time": "2018-06-19T16:22:30.070093Z"
    }
   },
   "outputs": [],
   "source": [
    "degree_centrality = convert_nx_output_to_dataframe(nx.degree_centrality(g), 'degree_centrality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T16:22:59.520171Z",
     "start_time": "2018-06-19T16:22:59.281473Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(7, 4))\n",
    "degree_centrality.plot(kind='barh', ax=axes)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T02:21:53.205358Z",
     "start_time": "2018-06-19T02:21:53.184785Z"
    }
   },
   "outputs": [],
   "source": [
    "nx.clustering(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i GraphEditingTools.py\n",
    "%run -i GraphTools.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T01:31:49.483627Z",
     "start_time": "2018-06-19T01:31:49.477781Z"
    }
   },
   "outputs": [],
   "source": [
    "nx.to_dict_of_lists(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T00:04:40.182887Z",
     "start_time": "2018-06-19T00:04:39.823807Z"
    }
   },
   "outputs": [],
   "source": [
    "create_node_network('migraine', 'crps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T00:00:48.845578Z",
     "start_time": "2018-06-19T00:00:48.655257Z"
    }
   },
   "outputs": [],
   "source": [
    "b = load_tweet_id_data('migraine')\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T00:00:48.990248Z",
     "start_time": "2018-06-19T00:00:48.982705Z"
    }
   },
   "outputs": [],
   "source": [
    "b[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T00:00:54.224446Z",
     "start_time": "2018-06-19T00:00:54.103210Z"
    }
   },
   "outputs": [],
   "source": [
    "c = load_tweet_id_data('crps')\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T23:54:48.775438Z",
     "start_time": "2018-06-18T23:54:48.771329Z"
    }
   },
   "outputs": [],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T00:02:17.478441Z",
     "start_time": "2018-06-19T00:02:17.466737Z"
    }
   },
   "outputs": [],
   "source": [
    "c[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T00:03:00.663120Z",
     "start_time": "2018-06-19T00:03:00.635605Z"
    }
   },
   "outputs": [],
   "source": [
    "j = c[c.id.isin(b.id)]\n",
    "len(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T00:03:07.674283Z",
     "start_time": "2018-06-19T00:03:07.666439Z"
    }
   },
   "outputs": [],
   "source": [
    "j[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pos=nx.spring_layout(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T23:14:46.333677Z",
     "start_time": "2018-06-18T23:14:46.326230Z"
    }
   },
   "outputs": [],
   "source": [
    "experimentalTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T23:14:46.333677Z",
     "start_time": "2018-06-18T23:14:46.326230Z"
    }
   },
   "outputs": [],
   "source": [
    "experimentalTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T23:54:57.595514Z",
     "start_time": "2018-06-18T23:54:57.579244Z"
    }
   },
   "outputs": [],
   "source": [
    "d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal distribution of tweets with stored hashtags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
