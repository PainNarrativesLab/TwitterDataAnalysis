{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word map databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We need to be able to rapidly look at tweets \n",
    "# down to the 1, 2, and 3 gram level. Thus the \n",
    "# tools in this section notebook create and \n",
    "# maintain the necessary databases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T21:52:26.757423Z",
     "start_time": "2018-02-16T21:52:25.293910Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "%cd twitteranalysis\n",
    "%run -i environment.py\n",
    "\n",
    "# TEST = False\n",
    "\n",
    "#General tools\n",
    "import sys\n",
    "import locale\n",
    "import json\n",
    "import time\n",
    "from random import shuffle\n",
    "import itertools #For set operations\n",
    "# from urllib2 import URLError\n",
    "import datetime\n",
    "from datetime import time\n",
    "from datetime import date\n",
    "import string\n",
    "import shelve\n",
    "\n",
    "#Data storage\n",
    "import redis\n",
    "import couchdb\n",
    "from couchdb.design import ViewDefinition\n",
    "\n",
    "#Display\n",
    "from CustomDisplayTools import Table\n",
    "from IPython.display import HTML\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Pandas\n",
    "from pandas import DataFrame, Series\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999 #let pandas dataframe listings go long\n",
    "\n",
    "#Network x\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite as bi\n",
    "\n",
    "\n",
    "#Plotting \n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Timing etc tools (e.g., @timefn)\n",
    "from OptimizationTools import *\n",
    "\n",
    "#SqlAlchemy\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Twitter specific\n",
    "%cd twittermining\n",
    "import twitter\n",
    "# import twitter_text # easy_install twitter-text-py\n",
    "# from TwitterLogin import login\n",
    "# # from TwitterUtilities import makeTwitterRequest\n",
    "# # t = login()\n",
    "# import TwitterSQLService\n",
    "# #%%bash couchdb\n",
    "# import TwitterServiceClasses as TSC\n",
    "# import TwitterSearcher as TS\n",
    "# import TwitterDataProcessors as TDP\n",
    "# #Data manipulation and cleaning\n",
    "# from TwitterDataProcessors import Tags\n",
    "# from TwitterDataProcessors import Extractors\n",
    "# from TwitterDataProcessors import TagsFromSearch\n",
    "%cd twitteranalysis\n",
    "import sphinxapi\n",
    "\n",
    "# Create your connection.\n",
    "\n",
    "# engine = create_engine('mysql://root:''@localhost:3306/twitter_wordsTEST')\n",
    "#engine = create_engine('mysql://testuser:testpass@localhost/twitter_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#To do\n",
    "- Build in text encoding normalization to the tweet object as a getter so that accessing tweet.text will yield expected encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T00:05:40.822891Z",
     "start_time": "2018-03-22T00:05:40.810976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'a'), (1, 'b')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['a', 'b']\n",
    "[(idx, txt) for idx, txt in enumerate(sentences)]\n",
    "# for idx, txt in enumerate(sentences):\n",
    "#     print(idx, txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process into single word database\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T21:52:27.640483Z",
     "start_time": "2018-02-16T21:52:26.777888Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Create tables in database\n",
    "from DataTools.WordORM import create_db_tables\n",
    "create_db_tables()\n",
    "\n",
    "#%cd /Users/adam/Dropbox/PainNarrativesLab/TwitterDataAnalysis\n",
    "#%bookmark twitteranalysis\n",
    "%cd twitteranalysis\n",
    "%run -i environment.py\n",
    "%run -i ConstantsAndUtilities.py\n",
    "%run -i TestingTools/DataAndFunctionsForTesting.py\n",
    "%run -i \"DataTools/DataStructures.py\"\n",
    "%run -i \"DataTools/DataConnections.py\"\n",
    "%run -i \"DataTools/WordORM.py\"\n",
    "%run -i \"DataTools/DataRepositories.py\" \n",
    "\n",
    "# Initialize the tools for filtering and modifying the individual tweet words\n",
    "from TextProcessors.Filters import *\n",
    "from TextProcessors.Modifiers import *\n",
    "%cd texttools\n",
    "%run -i TextProcessors/Processors\n",
    "%cd twitteranalysis\n",
    "%run -i ConstantsAndUtilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = session.query(Word).filter( Word.word == 'taco1' ).first( )\n",
    "# if isinstance(word, Word):\n",
    "#     print ('j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql cursor\n",
    "# # get a connection\n",
    "# import sqlalchemy.pool as pool\n",
    "#  'mysql://root:''@localhost:3306/twitter_wordsTEST' \n",
    "# def getconn():\n",
    "#     c = psycopg2.connect(username='ed', host='127.0.0.1', dbname='test')\n",
    "#     return c\n",
    "\n",
    "# mypool = pool.QueuePool(getconn, max_overflow=10, pool_size=5)\n",
    "# conn = mypool.connect()\n",
    "\n",
    "# # use it\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute(\"select foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(0, 10):\n",
    "#     print(cursor.next_tweet().tweetID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session.rollback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T21:53:45.014512Z",
     "start_time": "2018-02-16T21:53:44.834038Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd twitteranalysis\n",
    "\n",
    "%run -i DataTools/Cursors\n",
    "u = UserCursor(language='fr')\n",
    "t = TweetCursor( )\n",
    "\n",
    "k = u.next()\n",
    "k.lang, k.userID\n",
    "\n",
    "k = t.next()\n",
    "k.tweetID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T21:54:23.630742Z",
     "start_time": "2018-02-16T21:54:23.626598Z"
    }
   },
   "outputs": [],
   "source": [
    "k.tweetText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process actual tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-16T22:52:46.092Z"
    }
   },
   "outputs": [],
   "source": [
    "%cd twitteranalysis\n",
    "\n",
    "# %run -i profiling/optimizing_string_processor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-16T21:57:41.700Z"
    }
   },
   "outputs": [],
   "source": [
    "# from threading import Thread\n",
    "# # session.rollback()\n",
    "# # session.no_autoflush()\n",
    "\n",
    "# RUNS = 5\n",
    "# MAX_THREADS = 71\n",
    "# START_THREADS= 69\n",
    "# DATASTRUCTURES = ('tuple', 'dawg')\n",
    "# NOTE = 'programmatic run 69-71 threads'\n",
    "# MODULE = 'stringProcessingWorker'\n",
    "# NUM_TWEETS = 1181 #don't rely on other stuff\n",
    "\n",
    "# def add_run(frame, module, threads, tweets, time, datastructure, note):\n",
    "#     \"\"\"Adds a record to benchmarks\"\"\"\n",
    "#     d = {\n",
    "#         'id': frame.id.max() + 1,\n",
    "#         'timestamp': datetime.datetime.now(),\n",
    "#         'module': module,\n",
    "#         'numThreads': threads,\n",
    "#         'numTweets': tweets,\n",
    "#         'totalTime': time,\n",
    "#         'tweetTime': time / tweets,\n",
    "#         'dataStructure': datastructure,\n",
    "#         'note': note\n",
    "#     }\n",
    "#     d = pd.DataFrame([d])\n",
    "#     frame = pd.concat([frame, d])\n",
    "#     return frame\n",
    "\n",
    "# benchmarks = pd.read_csv(\"%s/tests/benchmarks/StringProcessingWorker.csv\" % TEXT_TOOLS_PATH)\n",
    "\n",
    "# for THREADS in range(START_THREADS, MAX_THREADS):\n",
    "#     for DATASTRUCTURE in DATASTRUCTURES:\n",
    "#         for _ in range(RUNS):\n",
    "#             print(\"threads: %s   datastructure: %s   run: %s\" % (THREADS, DATASTRUCTURE, _))\n",
    "\n",
    "#             # Create queue and listeners for processed tokens\n",
    "#             Queue = SaveQueueHandler()\n",
    "#             Queue.register_listener(SaveListener())\n",
    "\n",
    "#             # Load cursor for tweet ids\n",
    "#             cursor = DataTools.Cursors.TweetCursor()\n",
    "\n",
    "#             #Add regular or dawg filter\n",
    "#             if DATASTRUCTURE is 'dawg':\n",
    "#                 print('dawg')\n",
    "#                 word_processor.add_to_filters( ignoreDawgFilter )\n",
    "#             else:\n",
    "#                 word_processor.add_to_filters( ignoreListFilter )\n",
    "                \n",
    "#             #Initialize everything we need\n",
    "#             StringProcessingWorker.initialize(cursor, Queue, word_processor)\n",
    "#             threads = []\n",
    "#             time = Timer()\n",
    "\n",
    "#             #Go!\n",
    "#             time.start() \n",
    "\n",
    "#             if THREADS is 0:\n",
    "#                 worker = StringProcessingWorker()\n",
    "\n",
    "# #                 if DATASTRUCTURE is 'dawg':\n",
    "# #                     worker.do_it_dawg()\n",
    "# #                 else:\n",
    "#                 worker.do_it()\n",
    "\n",
    "#             else:\n",
    "#                 # if this run is using threads, we go this way\n",
    "#                 for _ in range(THREADS):\n",
    "#                     worker = StringProcessingWorker()\n",
    "                    \n",
    "# #                     if DATASTRUCTURE is 'dawg':\n",
    "# #                         thread = Thread(target=worker.do_it_dawg)\n",
    "# #                     else:\n",
    "#                     thread = Thread(target=worker.do_it)\n",
    "#                     threads.append(thread)\n",
    "#                     thread.start()\n",
    "\n",
    "#                 for t in threads:\n",
    "#                     t.join()\n",
    "\n",
    "#             #....and stop!\n",
    "#             time.stop() \n",
    "\n",
    "#             #Good job! Now, write it down and get back out there, tiger!\n",
    "#             benchmarks = add_run(frame=benchmarks, \n",
    "#                                  module=MODULE, \n",
    "#                                  threads=THREADS, \n",
    "#                                  tweets=NUM_TWEETS, \n",
    "#                                  time=time.elapsed_seconds, \n",
    "#                                  datastructure=DATASTRUCTURE, \n",
    "#                                  note=NOTE)\n",
    "#             benchmarks.set_index(['id']).to_csv(\"%s/tests/benchmarks/StringProcessingWorker.csv\" % TEXT_TOOLS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THREADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#benchmarks.timestamp = pd.to_datetime(benchmarks.timestamp)\n",
    "benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker.totalProcessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T23:42:49.762939Z",
     "start_time": "2018-02-16T23:42:49.679995Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-078bfc2f939e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s/tests/benchmarks/StringProcessingWorker.csv\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mTEXT_TOOLS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#data.tweetTime = data.totalTime / data.numTweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# data.drop(['date', 'module', 'note', 'time', 'numTweets', 'timestamp'], axis=1, inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#load from file\n",
    "data = pd.read_csv(\"%s/tests/benchmarks/StringProcessingWorker.csv\" % TEXT_TOOLS_PATH)\n",
    "data.set_index(['id'], inplace=True)\n",
    "#data.tweetTime = data.totalTime / data.numTweets\n",
    "# data.drop(['date', 'module', 'note', 'time', 'numTweets', 'timestamp'], axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key will be adding a dawg to hold the word ids so don't have to hit the db with every word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_times(totalProcessed, elapsed_seconds):\n",
    "    tweets_to_process = [690000, 4000000]\n",
    "    print(\"Processed %s tweets in %s seconds\" % (totalProcessed, elapsed_seconds))\n",
    "    time_per_tweet = elapsed_seconds/totalProcessed\n",
    "    for num in tweets_to_process:\n",
    "        estimated_sec = time_per_tweet * num\n",
    "        print(\"estimated time to process %s tweets: %s minutes \\n\" % (num, int( estimated_sec/60 ) ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ProcessingTools.Workers.StringProcessingWorker\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "time = Timer()\n",
    "time.start()\n",
    "\n",
    "# Create queue and listeners for processed tokens\n",
    "Queue = SaveQueueHandler()\n",
    "Queue.register_listener(SaveListener())\n",
    "\n",
    "# Load cursor for tweet ids\n",
    "cursor = DataTools.Cursors.TweetCursor()\n",
    "\n",
    "StringProcessingWorker.initialize(cursor, Queue, word_processor)\n",
    "threads = []\n",
    "\n",
    "# for _ in range(1):\n",
    "worker = StringProcessingWorker()\n",
    "worker.do_it()\n",
    "\n",
    "# thread = Thread(target=worker.do_it)\n",
    "# threads.append(thread)\n",
    "# thread.start()\n",
    "\n",
    "# for t in threads:\n",
    "#     t.join()\n",
    "\n",
    "time.stop()\n",
    "\n",
    "\n",
    "estimate_times(worker.totalProcessed, time.elapsed_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks['hours'] = benchmarks.est_4m_min.apply(lambda x: x/60)\n",
    "benchmarks.groupby('dataStructure').hours.plot(legend=True, title='Est hours for 4 million tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dawg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T03:01:22.828846Z",
     "start_time": "2018-02-17T03:01:22.731483Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0d0c76ec60fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdawg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDataTools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataStructures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# base_dawg = dawg.DAWG(words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0d0c76ec60fb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdawg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDataTools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataStructures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# base_dawg = dawg.DAWG(words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_result' is not defined"
     ]
    }
   ],
   "source": [
    "import dawg\n",
    "from DataTools import DataStructures\n",
    "words = [('text %s' % i, tuple(make_result(i, i, 'text %s' % i, i))) for i in range(0, 5)]\n",
    "words\n",
    "# base_dawg = dawg.DAWG(words)\n",
    "# completion_dawg = dawg.CompletionDAWG(words)\n",
    "\n",
    "# record_dawg = dawg.RecordDAWG(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T03:08:24.823197Z",
     "start_time": "2018-02-17T03:08:24.807840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text0', (0, 0, 'text0', 0)), ('text1', (1, 1, 'text1', 1)), ('text2', (2, 2, 'text2', 2)), ('text3', (3, 3, 'text3', 3)), ('text4', (4, 4, 'text4', 4))]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "bad char in struct format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5e8fdbe818b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrecord_dawg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdawg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecordDAWG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mdawg.pyx\u001b[0m in \u001b[0;36mdawg.RecordDAWG.__init__ (src/dawg.cpp:13810)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: bad char in struct format"
     ]
    }
   ],
   "source": [
    "format = DataStructures.Result\n",
    "keys = [u'text%s' % i for i in range(0, 5)]\n",
    "values = [(i, i, keys[i], i) for i in range(0, 5)]\n",
    "data = zip(keys, values)\n",
    "print([r for r in data])\n",
    "record_dawg = dawg.RecordDAWG(format, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T03:10:49.952618Z",
     "start_time": "2018-02-17T03:10:49.947389Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [ (u'foo', 1), (u'bar', 2) ]\n",
    "int_dawg = dawg.IntDAWG(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_dawg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T03:11:09.163633Z",
     "start_time": "2018-02-17T03:11:09.156023Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dawg.IntDAWG' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cac25d1f1f00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mint_dawg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dawg.IntDAWG' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "[print (k) for k in int_dawg.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in record_dawg.items():\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'text0' in record_dawg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print (k) for k in record_dawg.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u'foo' in base_dawg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_dawg.keys(u'foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u'baz' in completion_dawg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to test whether some key begins with a given prefix:\n",
    "\n",
    ">>> completion_dawg.has_keys_with_prefix(u'foo')\n",
    ">>> True\n",
    "\n",
    "and to find all prefixes of a given key:\n",
    "\n",
    ">>> base_dawg.prefixes(u'foobarz')\n",
    "[u'foo', u'foobar']\n",
    "\n",
    "Iterator versions are also available:\n",
    "\n",
    ">>> for key in completion_dawg.iterkeys(u'foo'):\n",
    "...     print(key)\n",
    "foo\n",
    "foobar\n",
    ">>> for prefix in base_dawg.iterprefixes(u'foobarz'):\n",
    "...     print(prefix)\n",
    "foo\n",
    "foobar\n",
    "\n",
    "It is possible to find all keys similar to a given key (using a one-way char translation table):\n",
    "\n",
    ">>> replaces = dawg.DAWG.compile_replaces({u'o': u'ö'})\n",
    ">>> base_dawg.similar_keys(u'foo', replaces)\n",
    "[u'foo', u'foö']\n",
    ">>> base_dawg.similar_keys(u'foö', replaces)\n",
    "[u'foö']\n",
    ">>> base_dawg.similar_keys(u'bor', replaces)\n",
    "[u'bör']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> keys = [u'foo', u'bar', u'foobar', u'foo']\n",
    ">>> values = [(1, 2, 3), (2, 1, 0), (3, 3, 3), (2, 1, 5)]\n",
    ">>> data = zip(keys, values)\n",
    ">>> record_dawg = RecordDAWG(format, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimate_times(totalProcessed, time.elapsed_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2090/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocesses setup\n",
    "import subprocess\n",
    "\n",
    "proc = subprocess.Popen(['echo', 'jjjj'], stdout=subprocess.PIPE)\n",
    "\n",
    "out, err = proc.communicate()\n",
    "print(out.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = Session()\n",
    "\n",
    "# w = session.query(Word).limit(1).all()\n",
    "# ww = w[0].word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ww.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = \"b'jip\"\n",
    "j[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dawg 1 thread\n",
    "Processed 1181 tweets in 33.9468 seconds\n",
    "\n",
    "estimated time to process 690000 tweets: 331 minutes \n",
    "\n",
    "estimated time to process 4000000 tweets: 1916 minutes \n",
    "\n",
    "# dawg no thread\n",
    "Processed 1181 tweets in 34.3619 seconds\n",
    "\n",
    "estimated time to process 690000 tweets: 334 minutes \n",
    "\n",
    "estimated time to process 4000000 tweets: 1939 minutes\n",
    "\n",
    "# reg 1 thread\n",
    "Processed 1181 tweets in 34.9496 seconds\n",
    "\n",
    "estimated time to process 690000 tweets: 340 minutes \n",
    "\n",
    "estimated time to process 4000000 tweets: 1973 minutes \n",
    "\n",
    "## run 2\n",
    "Processed 1181 tweets in 33.6475 seconds\n",
    "\n",
    "estimated time to process 690000 tweets: 327 minutes \n",
    "\n",
    "estimated time to process 4000000 tweets: 1899 minutes \n",
    "\n",
    "\n",
    "# reg no thread\n",
    "Processed 1181 tweets in 34.4375 seconds\n",
    "\n",
    "estimated time to process 690000 tweets: 335 minutes \n",
    "\n",
    "estimated time to process 4000000 tweets: 1943 minutes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T21:57:35.778534Z",
     "start_time": "2018-02-16T21:57:35.684845Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %run -i ProcessingTools/ProcessingControllers.py\n",
    "# %run -i ProcessingTools/Listeners.py\n",
    "# %run -i ProcessingTools/Workers.py\n",
    "\n",
    "\n",
    "# import DataTools.Cursors\n",
    "# #Mission Control\n",
    "\n",
    "# # Create queue and listeners for processed tokens\n",
    "# Queue = SaveQueueHandler()\n",
    "# Queue.register_listener(SaveListener())\n",
    "\n",
    "# # Create controller which will oversee the process\n",
    "# # control = TweetProcessingController(Queue)\n",
    "\n",
    "# # Initialize the tools for filtering and modifying the individual tweet words\n",
    "# word_processor = TextProcessors.Processors.SingleWordProcessor( )\n",
    "# ignore = ConstantsAndUtilities.Ignore( )\n",
    "# ignore._construct()\n",
    "# merge = ConstantsAndUtilities.Merge( )\n",
    "\n",
    "# #reg filter\n",
    "# ignoreListFilter = TextProcessors.Filters.IgnoreListFilter( )\n",
    "# ignoreListFilter.add_to_ignorelist( ignore.get_list( ) )\n",
    "# ignoreListFilter.add_to_ignorelist( nltk.corpus.stopwords.words( 'english' ) ) # or do we keep them?\n",
    "\n",
    "# #dawg filter\n",
    "# ignoreDawgFilter = TextProcessors.Filters.IgnoreDawgFilter()\n",
    "# ignoreDawgFilter.add_to_ignorelist( ignore.get_list( ) )\n",
    "# ignoreDawgFilter.add_to_ignorelist( nltk.corpus.stopwords.words( 'english' ) ) # or do we keep them?\n",
    "\n",
    "# word_processor.add_to_filters( TextProcessors.Filters.UsernameFilter( ) )\n",
    "# word_processor.add_to_filters( TextProcessors.Filters.PunctuationFilter( ) )\n",
    "# word_processor.add_to_filters( TextProcessors.Filters.URLFilter( ) )\n",
    "# word_processor.add_to_filters( TextProcessors.Filters.NumeralFilter( ) )\n",
    "# word_processor.add_to_modifiers( TextProcessors.Modifiers.WierdBPrefixConverter() )\n",
    "# # processor.add_to_modifiers( TextProcessors.Modifiers.UnicodeConverter() )\n",
    "# word_processor.add_to_modifiers( TextProcessors.Modifiers.CaseConverter( ) )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "198px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
