{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### This notebook is for analysis performed upon the tweets. \n",
    "# Note added 2018-02-16: Seems to have been a working set of tools here which may be\n",
    "# adapted to our use.... Not sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T21:46:12.082425Z",
     "start_time": "2018-02-16T21:46:10.348731Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:twitteranalysis) -> /Users/adam/Dropbox/PainNarrativesLab/TwitterDataAnalysis\n",
      "/Users/adam/Dropbox/PainNarrativesLab/TwitterDataAnalysis\n",
      "(bookmark:twittermining) -> /Users/adam/Dropbox/PainNarrativesLab/TwitterMining\n",
      "/Users/adam/Dropbox/PainNarrativesLab/TwitterMining\n",
      "(bookmark:twitteranalysis) -> /Users/adam/Dropbox/PainNarrativesLab/TwitterDataAnalysis\n",
      "/Users/adam/Dropbox/PainNarrativesLab/TwitterDataAnalysis\n"
     ]
    }
   ],
   "source": [
    "%cd twitteranalysis\n",
    "from environment import *\n",
    "\n",
    "#General tools\n",
    "import sys\n",
    "import locale\n",
    "import json\n",
    "import time\n",
    "from random import shuffle\n",
    "import itertools #For set operations\n",
    "# from urllib2 import URLError\n",
    "from datetime import time\n",
    "from datetime import date\n",
    "import string\n",
    "import shelve\n",
    "\n",
    "#Data storage\n",
    "import redis\n",
    "import couchdb\n",
    "from couchdb.design import ViewDefinition\n",
    "\n",
    "#Display\n",
    "from CustomDisplayTools import Table\n",
    "from IPython.display import HTML\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "\n",
    "#Network x\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite as bi\n",
    "\n",
    "# Pandas\n",
    "from pandas import DataFrame, Series\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999 #let pandas dataframe listings go long\n",
    "\n",
    "\n",
    "#Plotting \n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#Twitter specific\n",
    "%cd twittermining\n",
    "import twitter\n",
    "# import twitter_text # easy_install twitter-text-py\n",
    "# from TwitterLogin import login\n",
    "# # from TwitterUtilities import makeTwitterRequest\n",
    "# # t = login()\n",
    "# import TwitterSQLService\n",
    "# #%%bash couchdb\n",
    "# import TwitterServiceClasses as TSC\n",
    "# import TwitterSearcher as TS\n",
    "# import TwitterDataProcessors as TDP\n",
    "# #Data manipulation and cleaning\n",
    "# from TwitterDataProcessors import Tags\n",
    "# from TwitterDataProcessors import Extractors\n",
    "# from TwitterDataProcessors import TagsFromSearch\n",
    "%cd twitteranalysis\n",
    "\n",
    "#SqlAlchemy\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import sphinxapi\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create your connection.\n",
    "\n",
    "# engine = create_engine('mysql://root:''@localhost:3306/twitter_data')\n",
    "#engine = create_engine('mysql://testuser:testpass@localhost/twitter_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T21:47:21.246158Z",
     "start_time": "2018-02-16T21:47:21.233244Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'TweetDAOs.py'` not found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:twitteranalysis) -> /Users/adam/Dropbox/PainNarrativesLab/TwitterDataAnalysis\n",
      "/Users/adam/Dropbox/PainNarrativesLab/TwitterDataAnalysis\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TweetTextGetter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-44281e06647c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'twitteranalysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TweetDAOs.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetTextGetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TweetTextGetter' is not defined"
     ]
    }
   ],
   "source": [
    "#Load tweets from db\n",
    "%cd twitteranalysis\n",
    "%run TweetDAOs.py\n",
    "loader = TweetTextGetter()\n",
    "tweets = loader.load_tweets()\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From couchdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##From couchdb\n",
    "from TwitterServiceClasses import CouchService\n",
    "\n",
    "tweetids = []\n",
    "hashtags = []\n",
    "tweet_tuples = []\n",
    "errors = []\n",
    "\n",
    "for tweet in CouchService('compiled').db.query(\"\"\"function(doc){emit (doc.id, doc.entities.hashtags);}\"\"\"):\n",
    "    tags = []\n",
    "    if tweet.value != None:\n",
    "        for tag in tweet.value:\n",
    "            try:\n",
    "                #Make lowercase and convert to string\n",
    "                tag_cleaned = str(tag['text'])\n",
    "                tag_cleaned = tag_cleaned.lower()\n",
    "                #Make tuple of tweetid and tag\n",
    "                tweet_tuples.append((tweet.key, tag_cleaned))\n",
    "                #Keep track of hashtags and tweetids\n",
    "                hashtags.append(tag_cleaned)\n",
    "                tweetids.append(tweet.key)\n",
    "            except:\n",
    "                errors.append(tweet.key)\n",
    "#Make the list of tweet ids unique\n",
    "tweetids = list(set(tweetids))\n",
    "#Make the list of hashtags unique\n",
    "hashtags = list(set(hashtags))\n",
    "\n",
    "print '%i errors in processing tweets' % len(errors)\n",
    "print '%i tweets successfully processed' % len(tweetids)\n",
    "print '%i hashtags identified' % len(hashtags)\n",
    "\n",
    "####From mysql\n",
    "\n",
    "##Loaded from mysql\n",
    "\n",
    "TEST = False; LOCAL = True\n",
    "\n",
    "class TweetTuples(TwitterSQLService.SQLService):\n",
    "    \"\"\"\n",
    "    Fetches all tuples for graph making from sql database\n",
    "    \"\"\"\n",
    "    def __init__(self, test=False, local=True):\n",
    "        TwitterSQLService.SQLService.__init__(self, test, local)\n",
    "        self.query = \"\"\"SELECT tweetID, h.hashtag FROM tweetsXtags t INNER JOIN hashtags h ON t.tagID = h.tagID\"\"\"\n",
    "        self.val = []\n",
    "        self.returnAll()\n",
    "        self.tweet_tuples = []\n",
    "        self.hashtags = [] \n",
    "        self.tweetids = []\n",
    "        for t in list(self.results):\n",
    "            #process hashtag\n",
    "            tag_cleaned = str(t['hashtag'])\n",
    "            tag_cleaned = tag_cleaned.lower()\n",
    "            self.hashtags.append(tag_cleaned)\n",
    "            #process tweetids\n",
    "            tweetid = t['tweetID']\n",
    "            self.tweetids.append(tweetid)\n",
    "            #process tuple\n",
    "            tweet_tuple = (tweetid, tag_cleaned)\n",
    "            self.tweet_tuples.append(tweet_tuple)\n",
    "        self.hashtags = list(set(self.hashtags)) #Make list of unique hashtags\n",
    "        self.tweetids = list(set(self.tweetids)) #Make list of unique ids\n",
    "        print \"%s tuples loaded \\n %s unique hashtags \\n %s unique ids\" % (len(self.tweet_tuples), len(self.hashtags), len(self.tweetids))\n",
    "        \n",
    "tt = TweetTuples(test=TEST, local=LOCAL)\n",
    "#Make the graph\n",
    "g = nx.Graph()\n",
    "g.add_edges_from(tt.tweet_tuples)\n",
    "print nx.info(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet text analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Holds words to ignore etc\n",
    "%run ConstantsAndUtilities.py\n",
    "ignore = Ignore()\n",
    "merge = Merge()\n",
    "\n",
    "#Timing etc tools (e.g., @timefn)\n",
    "%run OptimizationTools.py\n",
    "\n",
    "#Make wordbag\n",
    "start_time = time.time() \n",
    "%run TextTools.py\n",
    "bagmaker = TweetTextWordBagMaker()\n",
    "bagmaker.add_to_ignorelist(ignore.get_list())\n",
    "bagmaker.add_to_ignorelist(nltk.corpus.stopwords.words('english'))\n",
    "bagmaker.new_process(tweets)\n",
    "print \"Execution time:\", time.time()-start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#s = shelve.open('test_data/wordbag')\n",
    "#s['wordbag'] = bagmaker.masterbag\n",
    "#s.close()\n",
    "\n",
    "#s = shelve.open('test_data/tweet_tuples')\n",
    "#s['tweet_tuples'] = bagmaker.tweet_tuples\n",
    "#s.close()\n",
    "\n",
    "#s = shelve.open('test_data/word_freqs')\n",
    "#s['word_freqs'] = word_frequencies\n",
    "#s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run TextStats.py\n",
    "wf = WordFreq(bagmaker.masterbag)\n",
    "%timeit word_frequencies = wf.compute_frequency_of_words_in_bag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load graph files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Full graph\n",
    "#fullgraph = nx.read_gpickle('all_tweets_pickle')\n",
    "#Bigraph\n",
    "tweetnet = nx.read_gpickle('twitter_graph_data/bigraph_full_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning tools\n",
    "### Prune graph\n",
    "Prune out tags which are known to be irrelevant and the result of overly broad search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i GraphEditingTools.py\n",
    "%run -i GraphTools.py\n",
    "\n",
    "####Merge conditions\n",
    "Merge nodes containing abbreviations and other aliases for conditions\n",
    "\n",
    "\n",
    "#Merge nodes\n",
    "before = nx.info(tweetnet)\n",
    "tweetnet = merge_from_list(tweetnet)\n",
    "after = nx.info(tweetnet)\n",
    "print \"Before merge: \" + before + '\\n'\n",
    "print \"After merge: \" + after\n",
    "\n",
    "#Terms by category\n",
    "sports = ['bodybuilding', 'football', 'crossfit',  \n",
    "            'gym','fitness', 'keepmovingkeepfit', 'running', 'run', 'sport', 'sports', 'train', 'workout', 'yoga',  ]\n",
    "bodymod = ['piercing', 'tattoo', 'ink']\n",
    "days = ['sunday', 'friday', 'monday' ]\n",
    "\n",
    "fibromerged = merge_nodes(tweetnet, ['fibro', 'fibromyalgia'], 'Fibromyalgia')\n",
    "nx.info(fibromerged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make bi-modal networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make binet projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.info(tweetnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Save original files\n",
    "import TwitterGEXF as TG #custom gexf saver\n",
    "today = date.today()\n",
    "filename = 'twitter_graph_data/%s_tweet_bigraphFULL.gexf' % date.today()\n",
    "#TG.write_gexf(tweetnet, filename)\n",
    "\n",
    "##Make the projected graph\n",
    "#tweetnet = bi.weighted_projected_graph(g, tt.hashtags, ratio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph analysis\n",
    "\n",
    "One of the first questions is which tags are most highly connected to others? \n",
    "That is, we need to determine the *degree* of each tag, i.e., how many other distinct tags does it co-occur with at least once. Networkx provides an algorithim to calculate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_degrees = tweetnet.degree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the most popular tags, we need to sort the mapping (from nodes to degrees) which tag_degrees contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply the sorting function\n",
    "sorted_degree = sorted_degree_map(tag_degrees)\n",
    "#Top 10 hashtags by degree\n",
    "sorted_degree[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.hist(tag_degrees.values(), 50, normed=True) #Display histogram of node degrees in 100 bins\n",
    "h = plt.hist(tag_degrees.values(), 100) #Display histogram of node degrees in 100 bins\n",
    "plt.loglog(h[1][1:],h[0]) #Plot same histogram in log-log space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the hashtags are very unevenly distributed. It will thus help to prune away the tags which are only connected to one other tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prune maingraph\n",
    "\n",
    "By trimming we went from the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.info(tweetnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to the more managable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trimmed = prune_below_degree(tweetnet, 100)\n",
    "nx.info(trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trimmed_degrees = trimmed.degree() #sort by degree\n",
    "metrsorted_degree = sorted_degree_map(trimmed_degrees)\n",
    "plt.hist(trimmed_degrees.values(), 100) #Display histogram of node degrees in 100 bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prune more radically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decorator version\n",
    "def text_normalize(otherfunction, *args, **kwargs):\n",
    "    \"\"\"Decorator to capitalize the string as it goes by.\"\"\"\n",
    "    def j(*args, **kwargs):\n",
    "        assert(type(args[0]) is str)\n",
    "        args[0] = args[0].strip().capitalize()    \n",
    "        otherfunction(*args, **kwargs)\n",
    "    return j\n",
    "\n",
    "\n",
    "@text_normalize\n",
    "def f(t):\n",
    "    print(\"%s\" % t)\n",
    "\n",
    "f('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supertrimmed = prune_below_degree(tweetnet, 500)\n",
    "nx.info(supertrimmed)\n",
    "supertrimmed_degrees = supertrimmed.degree()\n",
    "plt.hist(supertrimmed_degrees.values(), 100) #Display histogram of node degrees in 100 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supertrimmed_sorted_degree = sorted_degree_map(supertrimmed_degrees)\n",
    "d = DataFrame(supertrimmed_sorted_degree)\n",
    "d.set_index([0], inplace=True)\n",
    "Table.display(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Egograph analysis\n",
    "####Make egographs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####Load egographs\n",
    "\n",
    "paingraph = load('2014-06-16', 'pain_egograph') #Load pain ego graph\n",
    "nx.info(paingraph)\n",
    "\n",
    "migrainegraph = load('2014-06-16', 'migraine_egograph')\n",
    "nx.info(migrainegraph)\n",
    "\n",
    "arthritisgraph = load('2014-06-16', 'arthritis_egograph')\n",
    "nx.info(arthritisgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Properties of egographs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "migrainegraph = prune_below_degree(migrainegraph, 10 )\n",
    "nx.info(migrainegraph)\n",
    "\n",
    "arthritisgraph = prune_below_degree(arthritisgraph, 10)\n",
    "nx.info(arthritisgraph)\n",
    "\n",
    "#Try running stats on ego graph with ego removed\n",
    "arthritis_sans_ego = arthritisgraph.copy() \n",
    "arthritis_sans_ego.remove_nodes_from(['arthritis'])\n",
    "nx.info(arthritis_sans_ego)\n",
    "\n",
    "arthritis_sans_weights = [math.log(edata['weight']) for f,t,edata in arthritis_sans_ego.edges(data=True)]\n",
    "arthritis_sans_weights = [x for x in arthritis_sans_weights if x > 2]\n",
    "plt.hist(arthritis_sans_weights, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arthritis_sans_weights[:5]\n",
    "\n",
    "trimmed_arth_sans_weights = [x for x in arthritis_sans_weights if x > 2]\n",
    "plt.hist(trimmed_arth_sans_weights, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#arthritis_edge_weights = [math.log(edata['weight']) for f,t,edata in arthritisgraph.edges(data=True)]\n",
    "arthritis_edge_weights = [edata['weight'] for f,t,edata in arthritisgraph.edges(data=True)]\n",
    "print \"count of arthritis edge weights: %s\" % len(arthritis_edge_weights)\n",
    "\n",
    "trimmed_arth_weights = [x for x in arthritis_edge_weights if x > 2]\n",
    "\n",
    "plt.hist(trimmed_arth_weights, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Measurements of graph properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#calc closeness centrality\n",
    "%run GraphCalcTools.py\n",
    "\n",
    "####Calculate and save measurements\n",
    "\n",
    "#Calc clustering coefficients\n",
    "calc_and_save_clustering_coefficient(arthritisgraph, 'arthritis')\n",
    "calc_and_save_clustering_coefficient(migrainegraph, 'migraine')\n",
    "#Calc closeness\n",
    "calc_and_save_closeness_centrality(arthritisgraph, 'arthritis')\n",
    "#Calc betweenness centrality\n",
    "calc_and_save_betweeneness_centrality(arthritisgraph, 'arthritis')\n",
    "calc_and_save_betweeneness_centrality(migrainegraph, 'migraine')\n",
    "\n",
    "####Load saved measurements\n",
    "\n",
    "#Load betweenness\n",
    "migraine_betweenness = load_betweenness_centrality('migraine', '2014-06-16')\n",
    "arthritis_betweenness = load_betweenness_centrality('arthritis', '2014-06-16')\n",
    "#Make into dataframe summarizing\n",
    "mterms = []; aterms = []\n",
    "[mterms.append(i[0]) for i in migraine_betweenness]\n",
    "[aterms.append(i[0]) for i in arthritis_betweenness]\n",
    "terms = set(mterms + aterms)\n",
    "#Convert to dictionaries\n",
    "mdict = dict(migraine_betweenness)\n",
    "adict = dict(arthritis_betweenness)\n",
    "#Make dataframe\n",
    "cc = []\n",
    "for t in terms:\n",
    "    cc.append({'term' : t, 'migraine' : mdict.get(t), 'arthritis' : adict.get(t)})\n",
    "betweenness = DataFrame(cc)\n",
    "betweenness.set_index(['term'], inplace=True)\n",
    "betweenness.dropna(how='all', inplace=True)\n",
    "\n",
    "#Load closeness\n",
    "migraine_closeness = load_closeness_centrality('migraine', '2014-06-16')\n",
    "arthritis_closeness = load_closeness_centrality('arthritis', '2014-06-16')\n",
    "#Make into dataframe summarizing\n",
    "mterms = []; aterms = []\n",
    "[mterms.append(i[0]) for i in migraine_closeness]\n",
    "[aterms.append(i[0]) for i in arthritis_closeness]\n",
    "terms = set(mterms + aterms)\n",
    "#Convert to dictionaries\n",
    "mdict = dict(migraine_closeness)\n",
    "adict = dict(arthritis_closeness)\n",
    "#Make dataframe\n",
    "cc = []\n",
    "for t in terms:\n",
    "    cc.append({'term' : t, 'migraine' : mdict.get(t), 'arthritis' : adict.get(t)})\n",
    "closeness = DataFrame(cc)\n",
    "closeness.set_index(['term'], inplace=True)\n",
    "closeness.dropna(how='all', inplace=True)\n",
    "\n",
    "closeness.hist(bins=100)\n",
    "\n",
    "d = dict(mdict.items() + adict.items())\n",
    "\n",
    "ba = DataFrame(betweenness.arthritis.copy())\n",
    "ba.dropna(inplace=True)\n",
    "ba.sort(columns=['arthritis'], ascending=False, inplace=True)\n",
    "#Sort migraine on betweenness\n",
    "bm = DataFrame(betweenness.migraine.copy())\n",
    "bm.dropna(inplace=True)\n",
    "bm.sort(columns=['migraine'], ascending=False, inplace=True)\n",
    "\n",
    "migraine_degrees = migrainegraph.degree() #sort by degree\n",
    "plt.hist(migraine_degrees.values(), 100)\n",
    "#migrainemetrsorted_degree = sorted_degree_map(migraine_degrees)\n",
    "\n",
    "###Properties of nodes and edges \n",
    "\n",
    "trimmed.get_edge_data('fibromyalgia', 'migraine')\n",
    "\n",
    "trimmed.get_edge_data('fibro', 'migraine')\n",
    "\n",
    "trimmed.get_edge_data('fibro', 'headache')\n",
    "\n",
    "nx.info(trimmed, 'tcot')\n",
    "\n",
    "nx.info(trimmed, 'ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Groups of substring uses\n",
    "There are a lot of tags which won't be counted if just look at, e.g., 'fibro'. For instance, 'fibrolife'. What to do about them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dreload(TwitterSQLService)\n",
    "\n",
    "QS = TwitterSQLService.QueryShell()\n",
    "\n",
    "likeFibro = QS.runquery(\"\"\"SELECT DISTINCT tweetID FROM tweetsXtags txt \n",
    "INNER JOIN hashtags h ON txt.tagID = h.tagID WHERE h.hashtag LIKE '%%fibro%%'\"\"\")\n",
    "likeFibro = [x['tweetID'] for x in likeFibro]\n",
    "#Get tweets which are fibro or fibromyalgia\n",
    "isFibro = QS.runquery(\"\"\"SELECT DISTINCT tweetID FROM tweetsXtags txt \n",
    "INNER JOIN hashtags h ON txt.tagID = h.tagID WHERE h.hashtag = %s OR h.hashtag = %s\"\"\", ['fibro', 'fibromyalgia'])\n",
    "isFibro = [x['tweetID'] for x in isFibro]\n",
    "notCotag = list(set(likeFibro) - set(isFibro))\n",
    "fibro = list(set(isFibro + notCotag))\n",
    "\n",
    "print \"\"\"%s tweets have hashtags containing the substring 'fibro' \\n \n",
    "%s hashtags are either 'fibro' or 'fibromyalgia \\n \n",
    "%s contain the substring but not the full tag\"\"\" %(len(likeFibro), len(isFibro), len(notCotag))\n",
    "\n",
    "likeMigraine = QS.runquery(\"\"\"SELECT DISTINCT tweetID FROM tweetsXtags txt \n",
    "INNER JOIN hashtags h ON txt.tagID = h.tagID WHERE h.hashtag LIKE '%%migraine%%'\"\"\")\n",
    "likeMigraine = [x['tweetID'] for x in likeMigraine]\n",
    "#Get tweets which are fibro or fibromyalgia\n",
    "isMigraine = QS.runquery(\"\"\"SELECT DISTINCT tweetID FROM tweetsXtags txt \n",
    "INNER JOIN hashtags h ON txt.tagID = h.tagID WHERE h.hashtag = %s OR h.hashtag = %s\"\"\", ['migraine', 'migraines'])\n",
    "isMigraine = [x['tweetID'] for x in isMigraine]\n",
    "notCotag = list(set(likeMigraine) - set(isMigraine))\n",
    "migraine = list(set(isMigraine + notCotag))\n",
    "\n",
    "print \"\"\"%s tweets have hashtags containing the substring 'migraine' \\n \n",
    "%s hashtags are either 'migraine' or 'migraines \\n \n",
    "%s contain the substring but not the full tag\"\"\" %(len(likeMigraine), len(isMigraine), len(notCotag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple hashtag frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT h.hashtag, count(txt.tweetID) AS tagFreq \n",
    "FROM tweetsXtags txt INNER JOIN hashtags h ON h.tagID = txt.tagID \n",
    "GROUP BY hashtag ORDER BY tagFreq DESC\"\"\"\n",
    "\n",
    "q = pd.read_sql_query(query, engine)\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Check whether tweets have been consistently gathered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_reformat(x):\n",
    "    s = x.split()\n",
    "    new = \"%s-%s-%s\" % (s[5], s[1], s[2])\n",
    "    return new\n",
    "QS = TwitterSQLService.QueryShell()   \n",
    "tweet_times = DataFrame(QS.runquery(\"\"\"SELECT created_at AS DayCollected FROM tweets\"\"\"))\n",
    "tt = DataFrame(tweet_times['DayCollected'].apply(date_reformat))\n",
    "ttg = tt.groupby('DayCollected').DayCollected.count()\n",
    "ttg.plot(x_compat=True, figsize=(15,5), title=\"New tweetIDs collected by day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "235px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
