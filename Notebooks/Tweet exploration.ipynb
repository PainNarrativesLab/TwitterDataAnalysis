{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for exploring the tweet data, independent of user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:44:07.636364Z",
     "start_time": "2018-06-12T21:44:06.049564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bookmark:twitteranalysis) -> /Users/adam/Dropbox/PainNarrativesLab/TwitterDataAnalysis\n",
      "/Users/adam/Dropbox/PainNarrativesLab/TwitterDataAnalysis\n"
     ]
    }
   ],
   "source": [
    "%cd twitteranalysis\n",
    "import pandas as pd\n",
    "#Plotting \n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "import environment\n",
    "from DataTools import DataRepositories as DR\n",
    "from DataTools import DataConnections as DC\n",
    "from DataTools.WordORM import Word\n",
    "from DataTools.TweetORM import Users as User\n",
    "from DataTools.TweetORM import Tweet\n",
    "from SearchTools.WordMaps import get_adjacent_word_counts, get_adjacent_words, get_tweet_ids_for_word\n",
    "\n",
    "EXP_TERMS_FILEPATH = '%s/experimental-terms.xlsx' % environment.EXPERIMENTS_FOLDER\n",
    "IDS_FILEPATH = \"%s/temp_output/tweet-ids.csv\" % environment.LOG_FOLDER_PATH\n",
    "\n",
    "def make_term_ids_filepath(term, path=environment.LOG_FOLDER_PATH):\n",
    "    return \"%s/temp_output/tweet-ids/%s-ids.csv\" % (path, term)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:44:07.673878Z",
     "start_time": "2018-06-12T21:44:07.638124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find users whose profile contains an experimental term\n",
    "\n",
    "\n",
    "def get_rows_for_terms(wordFrame, experimentalTerms):\n",
    "    return wordFrame[wordFrame.term.isin(experimentalTerms)]\n",
    "\n",
    "\n",
    "def find_mapping(term, termMap):\n",
    "    for t in termMap.T.index:\n",
    "        if termMap[t].str.contains(term).any():\n",
    "            return t\n",
    "    return False\n",
    "\n",
    "\n",
    "# load in terms to search for\n",
    "experimentalTerms = pd.read_excel(EXP_TERMS_FILEPATH, sheet_name='terms', squeeze=True)\n",
    "termMap = pd.read_excel(EXP_TERMS_FILEPATH, sheet_name='mapping')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:44:07.678932Z",
     "start_time": "2018-06-12T21:44:07.676011Z"
    }
   },
   "outputs": [],
   "source": [
    "def term_map_generator(termMap):\n",
    "    for term in termMap.T.index:\n",
    "        for subterm in termMap[term]:\n",
    "            yield (term, subterm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:35:47.591018Z",
     "start_time": "2018-06-12T21:35:47.588304Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = term_map_generator(termMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:35:48.080817Z",
     "start_time": "2018-06-12T21:35:48.075911Z"
    }
   },
   "outputs": [],
   "source": [
    "next(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many tweets contain each term?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T19:49:55.317051Z",
     "start_time": "2018-06-12T19:42:44.943062Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# iterate through each of the experimental terms, \n",
    "# getting tweet ids for each.\n",
    "\n",
    "# ids = []\n",
    "for t in termMap.T.index:\n",
    "    tweets = []\n",
    "    for subterm in termMap[t]:\n",
    "        tweets += [x[0] for x in get_tweet_ids_for_word(subterm)]\n",
    "    tweets = list(set(tweets))\n",
    "    tweets = pd.Series(tweets, name=t)\n",
    "\n",
    "    print(\"%s : %s\" % (t, len(tweets)))\n",
    "    tweets.to_csv(make_term_ids_filepath(t)) \n",
    "\n",
    "    \n",
    "# ids = pd.DataFrame(ids)\n",
    "\n",
    "# # Save results\n",
    "# ids.T.to_csv(IDS_FILEPATH) \n",
    "\n",
    "# len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T19:32:21.927468Z",
     "start_time": "2018-06-12T19:32:01.188397Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "tweets = [x[0] for x in get_tweet_ids_for_word('migraine')]\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T19:34:16.938175Z",
     "start_time": "2018-06-12T19:34:14.639541Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "ids = tweets\n",
    "ids = pd.DataFrame(ids)\n",
    "\n",
    "ids.T.to_csv(make_term_ids_filepath('migraine')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many unique users have had at least one tweet using the term captured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:46:55.724512Z",
     "start_time": "2018-06-12T21:46:55.696882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating connection: mysql twitter_data\n"
     ]
    }
   ],
   "source": [
    "e = DC.initialize_engine('mysql')\n",
    "dao = DC.DAO(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:46:45.255888Z",
     "start_time": "2018-06-12T21:46:45.252527Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_user_id_and_timestamp_for_tweet(tweetId):\n",
    "    \"\"\"Given a tweet id, this queries the mysql db and returns\n",
    "    a tuple of the user id and timestamp.\n",
    "    Returns: (userID, created_at)\n",
    "    \"\"\"\n",
    "    # get the tweet object\n",
    "    result = dao.session.query(Tweet).filter(Tweet.tweetID == tweetId).all()\n",
    "    return (result[0].userID, result[0].created_at)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tweet_ids_for_term_generator(term):\n",
    "    \"\"\"Generator for iterating over the ids of tweets which contain the term.\n",
    "    It loads the ids from the relevant file. It does not search the db.\n",
    "    \"\"\"\n",
    "    fp = make_term_ids_filepath(term)\n",
    "    print(fp)\n",
    "    # load the tweet ids for the term\n",
    "    ids = pd.read_csv(fp, squeeze=True)\n",
    "    for idx, row in ids.iterrows():\n",
    "        tweetId = row[1]\n",
    "        yield tweetId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:57:36.821520Z",
     "start_time": "2018-06-12T22:02:41.927656Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crps\n",
      "migraine\n",
      "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/migraine-ids.csv\n",
      "migraine : 549395 results \n",
      "fibromyalgia\n",
      "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/fibromyalgia-ids.csv\n",
      "fibromyalgia : 662498 results \n",
      "spoonie\n",
      "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/spoonie-ids.csv\n",
      "spoonie : 563467 results \n",
      "vulvodynia\n",
      "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/vulvodynia-ids.csv\n",
      "vulvodynia : 5611 results \n",
      "endometriosis\n",
      "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/endometriosis-ids.csv\n",
      "endometriosis : 298428 results \n",
      "neuropathy\n",
      "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/neuropathy-ids.csv\n",
      "neuropathy : 62210 results \n",
      "arthritis\n",
      "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/arthritis-ids.csv\n",
      "arthritis : 573767 results \n",
      "rhem_arthritis\n",
      "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/rhem_arthritis-ids.csv\n",
      "rhem_arthritis : 27342 results \n",
      "shingles\n",
      "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/shingles-ids.csv\n",
      "shingles : 2085 results \n",
      "backpain\n",
      "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/backpain-ids.csv\n",
      "backpain : 50053 results \n",
      "headache\n",
      "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/headache-ids.csv\n",
      "headache : 152961 results \n",
      "2947817\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fp_maker(term, path=environment.LOG_FOLDER_PATH):\n",
    "    return \"%s/temp_output/tweet-ids-w-user-ids/%s-ids.csv\" % (path, term)    \n",
    "\n",
    "passes = 0\n",
    "for term in termMap.T.index:\n",
    "        results = []\n",
    "        print(term)\n",
    "        if term != 'crps':\n",
    "            id_generator = tweet_ids_for_term_generator(term)\n",
    "            while True:\n",
    "                try:\n",
    "                    tweetId = next(id_generator)\n",
    "                    passes += 1\n",
    "                    userId, timestamp = get_user_id_and_timestamp_for_tweet(tweetId)\n",
    "                    results.append({'tweetId' : tweetId, 'userId': userId, 'timestamp' : timestamp })\n",
    "                except StopIteration:\n",
    "                    results = pd.DataFrame(results)\n",
    "                    results.to_csv(fp_maker(term))\n",
    "                    print('%s : %s results ' % (term, len(results)))\n",
    "                    break\n",
    "print(passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T00:23:17.663960Z",
     "start_time": "2018-06-13T00:23:13.266535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'crps': 52477,\n",
       " 'migraine': 158936,\n",
       " 'fibromyalgia': 99718,\n",
       " 'spoonie': 61915,\n",
       " 'vulvodynia': 1445,\n",
       " 'endometriosis': 56379,\n",
       " 'neuropathy': 13674,\n",
       " 'arthritis': 134146,\n",
       " 'rhem_arthritis': 8986,\n",
       " 'shingles': 1202,\n",
       " 'backpain': 12423,\n",
       " 'headache': 31892}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userCounts = {}\n",
    "\n",
    "# determine unique users for each term\n",
    "for term in termMap.T.index:\n",
    "    frame = pd.read_csv(fp_maker(term))\n",
    "    userCounts[term] = len(set(frame.userId.tolist()))\n",
    "\n",
    "userCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User counts -- at least one tweet using term by user\n",
    "{\n",
    "'crps': 52477,\n",
    " 'migraine': 158936,\n",
    " 'fibromyalgia': 99718,\n",
    " 'spoonie': 61915,\n",
    " 'vulvodynia': 1445,\n",
    " 'endometriosis': 56379,\n",
    " 'neuropathy': 13674,\n",
    " 'arthritis': 134146,\n",
    " 'rhem_arthritis': 8986,\n",
    " 'shingles': 1202,\n",
    " 'backpain': 12423,\n",
    " 'headache': 31892\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results \n",
    "\n",
    "crps\n",
    "migraine\n",
    "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/migraine-ids.csv\n",
    "migraine : 549395 results \n",
    "fibromyalgia\n",
    "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/fibromyalgia-ids.csv\n",
    "fibromyalgia : 662498 results \n",
    "spoonie\n",
    "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/spoonie-ids.csv\n",
    "spoonie : 563467 results \n",
    "vulvodynia\n",
    "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/vulvodynia-ids.csv\n",
    "vulvodynia : 5611 results \n",
    "endometriosis\n",
    "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/endometriosis-ids.csv\n",
    "endometriosis : 298428 results \n",
    "neuropathy\n",
    "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/neuropathy-ids.csv\n",
    "neuropathy : 62210 results \n",
    "arthritis\n",
    "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/arthritis-ids.csv\n",
    "arthritis : 573767 results \n",
    "rhem_arthritis\n",
    "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/rhem_arthritis-ids.csv\n",
    "rhem_arthritis : 27342 results \n",
    "shingles\n",
    "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/shingles-ids.csv\n",
    "shingles : 2085 results \n",
    "backpain\n",
    "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/backpain-ids.csv\n",
    "backpain : 50053 results \n",
    "headache\n",
    "/Users/adam/Desktop/TwitterDataAnalysisLogs/temp_output/tweet-ids/headache-ids.csv\n",
    "headache : 152961 results \n",
    "2947817\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:58:33.782880Z",
     "start_time": "2018-06-12T21:58:33.388600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310138"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.read_csv(fp_maker('crps'))\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:00:25.266388Z",
     "start_time": "2018-06-12T22:00:25.263687Z"
    }
   },
   "outputs": [],
   "source": [
    "seconds = (7* 60) + 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:00:25.736105Z",
     "start_time": "2018-06-12T22:00:25.733848Z"
    }
   },
   "outputs": [],
   "source": [
    "perRecord = 310138 / seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T22:00:26.323965Z",
     "start_time": "2018-06-12T22:00:26.319991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671.2943722943722"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
